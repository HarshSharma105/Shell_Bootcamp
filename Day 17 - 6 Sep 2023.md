
- Data flow : Transformation of Data
- Control flow : Control Flow Activity is **an activity that affects the path of execution of the Data Factory pipeline**. E.g. for each activity, which creates a loop if conditions are met.
- The following control activity types are available in ADF v2:
	- **Append Variable**: Append Variable activity could be used to add a value to an existing array variable defined in a Data Factory pipeline.
	- **Set Variable:** Set Variable activity can be used to set the value of an existing variable of type String, Bool, or Array defined in a Data Factory pipeline.
	- **Execute Pipeline**: The Execute Pipeline activity allows a Data Factory pipeline to invoke another pipeline.
	- **If Condition:** If Condition activity allows directing pipeline execution, based on evaluation of certain expressions.
	- **Get Metadata**: Get Metadata activity can be used to retrieve metadata of any data in Azure Data Factory.
	- **ForEach**: The ForEach activity defines a repeating control flow in your pipeline.
	- **Lookup:** Lookup activity can retrieve a dataset from any of the Azure Data Factory supported data sources.
	- **Filter**: Filter activity can be used in a pipeline to apply a filter expression to an input array.
	- **Until:** Until activity executes a set of activities in a loop until the condition associated with the activity evaluates to true.
	- **Wait:** Wait activity allows pausing pipeline execution for specified time period.
	- **Web:** Web activity can be used to call a custom REST endpoint from a Data Factory pipeline.
	- **WebHook** : A webhook activity can **control the execution of pipelines through custom code**. With the webhook activity, code can call an endpoint and pass it a callback URL.
	- **Azure Function:** The Azure Function activity allows you to run [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview) in a Data Factory pipeline.
- Within an pipeline we can not have more than 40 activities.
- If we have more than 40 activities, we can have *"parent-child"* pipelines i.e. have a execute pipeline activity. This is called *Chaining a pipeline*
- Activities speak to each other using *JSON* data.

### Sep 6 - Hands on Activity
- Download zipcodes file.
- Create ADLS Gen2 account
- Upload downloaded file in to 'input' container
- Create ADF Pipeline (PL_IDA_01)
- Follow proper naming convention eg: LS_ or DS_ 
- Create Copy Data Activity, copy file from input -> output container
#### **Parameterisation**:
- Create two pipeline parameters : PL_input, PL_output
- DS_input_param in DS_source, and DS_output_param in DS_destination
#### **Trigger**:
- Triggers in Azure Data Factory are objects that enable you to schedule and automate the execution of data pipelines and activities within your data workflows.
- **Types of Triggers:**
	- **Scheduled Triggers:** These are time-based triggers that allow you to set specific schedules for running your pipelines and activities. You can configure them to run at specific times, on a daily, weekly, or monthly basis.
	- **Tumbling Window Triggers:** These triggers are used to process data in recurring time windows, such as hourly, daily, or weekly. They are particularly useful for handling streaming or partitioned data. It is significantly more advantageous than Schedule Triggers when working with historical data to copy or migrate data.
	- **Event-Based Triggers:** Event-based triggers respond to events in external systems, like the arrival of a file in Azure Data Lake Storage or a message in Azure Service Bus. They initiate pipeline executions when the defined event occurs.
- **Use Cases of each type of Trigger:**
	- Scheduled triggers are commonly used for routine data processing tasks like ETL (Extract, Transform, Load) jobs that need to run at specific intervals.
	- Tumbling window triggers are suitable for scenarios where data is generated in batches and needs to be processed periodically.
	- Event-based triggers are helpful for responding to real-time events or data changes, such as processing files as they arrive in a storage account.
- Azure Data Factory provides monitoring and management capabilities for triggers, including the ability to view trigger history, status, and logs. This helps in tracking the execution and performance of triggered pipelines.
- Follow best practices for designing triggers to ensure efficient and reliable execution of your data workflows, such as setting appropriate time zones and handling time zone differences.

#### If-Condition Activity:
- used to introduce conditional logic within your Azure Data Factory (ADF) pipelines. It allows you to make decisions based on the evaluation of expressions or conditions.