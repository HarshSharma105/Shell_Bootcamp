
- Creating an blob storage and another ADLS Gen2 sotrage accounts.
- Uploading the files in the [retail](https://drive.google.com/drive/folders/1o44RQBb4muCzTbY93rd8bo1CfDyPx2a3?usp=sharing) folder in a new "retail" container inside the created blob storage account.
- Creating a Data Factory, inside the DF create a pipeline with "for each activity".
- Running a Data flow in pipeline needs a spark cluster(machine) for running the transformations.
- Source Transformation using data flow activity
- Basic transformations in DF's Data flows:
	- join
	- split
	- exists
	- union
	- lookup
	- derived column
	- select
	- aggregate
	- surrogate key
	- pivot
	- un-pivot
	- window
	- rank
	- flatten
	- parse
	- filter
	- sort
- using for each activity for looping over file names (provided by get metadata activity ran upon source container, not individual files), than doing a copy data activity to copy each file in source container to an destination container.
- ### Web Activity:
	- for get and post activity from an API.