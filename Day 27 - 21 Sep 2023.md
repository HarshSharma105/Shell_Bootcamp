# PySpark

- Works on the principle of Driver-Worker(Executors) paradigm.
- Spark supports Python (PySpark), Scala, R, and SQL.
- Components: Driver, Worker, Cluster Manager (CM)
- Divides files into chunks called partitions for Parallel processing by executors.
- By default splits files into 200 partitions.
- Lazy Evaluation: Transformation will be applied to a data frame only when some action is executed (like df.show, df.write).
- Driver creates Jobs
- Jobs created based on actions.
- Types of Transformations:
	- Narrow, and Wide
	- Narrow: straightforward transformations.
	- Wider: shuffle transformations, nodes communicate with each other.
- number of Shuffle decides number of Stages.
- Each Stage can have a number of Tasks.
- Tasks will be created based on number of input partition files
- RDD, Dataframe, and Dataset
- Adaptive Query Execution
- map
- flatmap
- filter
- reduceByKey
- groupByKey
- spark.read.csv()
- spark.read.option("inferSchema", True).option("header", True).csv(".....")
- from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType
- `schema = StructType([StructField("name", StringType(), nullable = True)])`
- Deployment Types:
	- Client Mode
	- Cluster Mode
- df.withColumn()
- lit(value) => for putting a default value for all rows