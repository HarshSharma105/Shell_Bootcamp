{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d051036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 05:50:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# caching\n",
    "# managed unmanaged table\n",
    "# persist, unpersist, levels\n",
    "# unstructured data - schema from json\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ShellIda25Sep2023\").getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c565b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3782b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "test_data = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "ud_scehma = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data=test_data ,schema = ud_scehma)\n",
    "\n",
    "# To solve the Spark Version Error:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a44f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|employee_name|   string|   null|\n",
      "|   department|   string|   null|\n",
      "|        state|   string|   null|\n",
      "|       salary|   bigint|   null|\n",
      "|          age|   bigint|   null|\n",
      "|        bonus|   bigint|   null|\n",
      "+-------------+---------+-------+\n",
      "\n",
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|       employee_name|              string|   null|\n",
      "|          department|              string|   null|\n",
      "|               state|              string|   null|\n",
      "|              salary|              bigint|   null|\n",
      "|                 age|              bigint|   null|\n",
      "|               bonus|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|             default|       |\n",
      "|               Table|             emp_tbl|       |\n",
      "|        Created Time|Mon Sep 25 05:50:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.4.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/home/labuse...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "\n",
    "test=StorageLevel(useDisk=False,useMemory=True,useOffHeap=False,deserialized=False)\n",
    "\n",
    "\n",
    "df.persist(storageLevel=test)\n",
    "\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "\n",
    "df.groupBy(\"department\")\n",
    "\n",
    "\n",
    "df.write.saveAsTable(\"emp_tbl\")\n",
    "\n",
    "\n",
    "test=spark.sql(\"DESCRIBE emp_tbl\")\n",
    "\n",
    "\n",
    "test.show()\n",
    "\n",
    "\n",
    "test01=spark.sql(\"DESCRIBE EXTENDED emp_tbl\")\n",
    "test01.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739f0cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE idashell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e1860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use idashell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b62af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"emp_etbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4fa29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|       employee_name|              string|   null|\n",
      "|          department|              string|   null|\n",
      "|               state|              string|   null|\n",
      "|              salary|              bigint|   null|\n",
      "|                 age|              bigint|   null|\n",
      "|               bonus|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|            idashell|       |\n",
      "|               Table|            emp_etbl|       |\n",
      "|        Created Time|Mon Sep 25 06:10:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.4.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/home/labuse...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test01 = spark.sql(\"DESCRIBE EXTENDED emp_etbl\")\n",
    "test01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a49a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff8194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(\"home/labuser/testcontainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb4fa9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"department\").csv(\"/home/labuser/Desktop/25Sep2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f74267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"state\",\"department\").csv(\"/home/labuser/Desktop/25Sep2023/groupByMany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe5d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finance5:>                                                          (0 + 2) / 2]\n",
      "Finance\n",
      "Finance\n",
      "Marketing\n",
      "Marketing\n",
      "Sales\n",
      "Sales\n",
      "Sales\n",
      "Finance\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for each\n",
    "\n",
    "def forEachDep(df):\n",
    "    print(df.department)\n",
    "    \n",
    "test = df.foreach(forEachDep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec12079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined schema from json\n",
    "\n",
    "# df = spark.read.schema(schema).json(spark.sparkContext.parallelize(json_data))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
